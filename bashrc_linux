# .bashrc

# User specific aliases and functions

alias rm='rm -i'
alias cp='cp -i'
alias mv='mv -i'
#alias python=/usr/local/bin/python2.7
alias xterm='xterm -bg black -fg white -geometry 72x34+100+40 -fn *-fixed-*-*-*-20-*'

# Source global definitions
if [ -f /etc/bashrc ]; then
	. /etc/bashrc
fi
alias vi=vim
alias puppetonce='puppet agent --onetime --verbose --no-daemonize'

export PS1="\[\033[36m\]\u\[\033[m\]@\[\033[32m\]\h:\[\033[33;1m\]\w\[\033[m\]\$ "
#export LS_COLORS="di=34;40:ln=35;40:so=32;40:pi=33;40:ex=31;40:bd=34;46:cd=34;43:su=0;41:sg=0;46:tw=0;42:ow=0;43:"	 #linux us LS_COLORS
alias ls='ls --color=auto'

export PATH=$PATH:/usr/libexec/nx/
export CUDA_INSTALL_PATH=/usr/local/cuda/
export PATH=$PATH:$CUDA_INSTALL_PATH/bin/
### this part for running on the IASTATE server ###
HOSTNAME=`hostname|grep -E "linux|research"`      ### careful, execute command in `` strings must use "" quote ###
if [ -n "$HOSTNAME" ]; then
    export PATH=$PATH:$HOME/gem5-gpu/cuda_components/bin/
fi
export NVIDIA_COMPUTE_SDK_LOCATION=$CUDA_INSTALL_PATH/NVIDIA_GPU_Computing_SDK/
export NVIDIA_CUDA_SDK_LOCATION=$NVIDIA_COMPUTE_SDK_LOCATION/C
export C_INCLUDE_PATH=/usr/include/openmpi-x86_64/
export CUDAHOME=$CUDA_INSTALL_PATH
# export LD_LIBRARY_PATH=$LD_LIBRARAYPATH:$CUDA_INSTALL_PATH/lib64/:$CUDA_INSTALL_PATH/lib/:$NVIDIA_CUDA_SDK_LOCATION/lib:/usr/lib64/:/usr/lib
export LD_LIBRARY_PATH=$LD_LIBRARAYPATH:$NVIDIA_CUDA_SDK_LOCATION/lib:/usr/lib64/:/usr/lib

################## Hadoop Environment Setup #################
# Set Hadoop-related environment variables
export HADOOP_INSTALL=/Downloads/hadoop
export HADOOP_COMMON_HOME=$HADOOP_INSTALL
export HADOOP_HDFS_HOME=$HADOOP_INSTALL
export HADOOP_YARN_HOME=$HADOOP_INSTALL
export HADOOP_MAPRED_HOME=$HADOOP_INSTALL


# Set JAVA_HOME (we will also configure JAVA_HOME directly for Hadoop later on)
export JAVA_HOME=/usr/lib/jvm/jdk

# Some convenient aliases and functions for running Hadoop-related commands
unalias hfs &> /dev/null
alias hfs="hadoop fs"
unalias hls &> /dev/null
alias hls="fs -ls"

# If you have LZO compression enabled in your Hadoop cluster and
# compress job outputs with LZOP (not covered in this tutorial):
# Conveniently inspect an LZOP compressed file from the command
# line; run via:
#
# $ lzohead /hdfs/path/to/lzop/compressed/file.lzo
#
# Requires installed 'lzop' command.
#
lzohead () {
    hadoop fs -cat $1 | lzop -dc | head -1000 | less
}

# Add Hadoop bin/ directory to PATH
export PATH=$PATH:$HADOOP_INSTALL/bin:$HADOOP_INSTALL/sbin

# Add M5_PATH
export M5_PATH=/Downloads/gem5-stable/x86_full_system_image/

#import new version texlive
export PATH=$PATH:/usr/local/texlive/2014/bin/x86_64-linux/

# export LANG="zh_CN.UTF-8"
# export LC_CTYPE="zh_CN.UTF-8"
# export XIM=fcitx
# export XIM_PROGRAM=fcitx
# export GTK_IM_MODULE=xim
# export XMODIFIERS=”@im=fcitx”
 # export GTK_IM_MODULE=ibus
 # export QT_IM_MODULE=ibus
 # export CLUTTER_IM_MODULE=ibus
# export GTK_IM_MODULE=xim 
# export QT_IM_MODULE=xim 
# export XMODIFIERS=@im=ibus
# ibus-daemon -drx
